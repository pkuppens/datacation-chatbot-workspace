{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From LLM to agent\n",
    "\n",
    "In this workshop, we will incrementally build a powerful agent-based chatbot.\n",
    "\n",
    "This notebook will guide you through the steps, but some parts are left as exercises for you to complete.\n",
    "\n",
    "> REMEMBER: The notebook is just there to experiment; the end result needs to be present in `chat_app.py` to be able to interactively chat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the basic chatbot\n",
    "\n",
    "We will begin by creating a basic chatbot using the `ChatGoogleGenerativeAI` model.\n",
    "\n",
    "> REMEMBER: The use of the Google API is configured by setting `GOOGLE_API_KEY=...` in a `.env` file in the root of the project.\n",
    "> To get an API key: https://aistudio.google.com/app/apikey\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "- Open the file `chat_app.py`.\n",
    "- Review the code and understand how the `on_message` function streams responses from the LLM.\n",
    "- Run the chatbot using the `main()` function and test it with some sample inputs.\n",
    "- Consider what the `AsyncLangchainCallbackHandler` does to monitor what's going on\n",
    "\n",
    "### Questions to consider:\n",
    "\n",
    "- How does the `astream` method work?\n",
    "- What is the role of `RunnableConfig`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-04-17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From LangChain to LangGraph\n",
    "\n",
    "To start building a proper agent, we need to migrate from LangChain to LangGraph.\n",
    "This will provide us with a thread-based message history with minimal configuration (the ability to ask follow-up questions).\n",
    "Ensure you have the proper dependencies (`uv add langgraph`).\n",
    "\n",
    "Check https://langchain-ai.github.io/langgraph/agents/agents/#basic-configuration\n",
    "\n",
    "> TIP: Add the current date into the prompt, as it will often think of the cutoff date as 'today'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from datetime import datetime\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[],  # We'll add tools later\n",
    "    prompt=f\"You are a helpful assistant. Today's date is {datetime.now().strftime('%Y-%m-%d')}.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiving responses from the agent\n",
    "\n",
    "LangGraph provides flexibility in how you can handle responses from the LLM. You can choose between implementing an **async streaming API** for a better user experience (as if the LLM types back) or processing responses synchronously for simplicity.\n",
    "\n",
    "### Async Streaming API\n",
    "\n",
    "The async streaming API allows you to stream responses incrementally as they are generated. This is particularly useful for creating a more interactive and responsive chatbot. To implement this:\n",
    "\n",
    "1. Use the `astream` method provided by LangGraph.\n",
    "2. Properly `await` the necessary coroutines.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "async for msg, metadata in agent.astream(\n",
    "    dict(messages=\"Some query\"),\n",
    "    stream_mode=\"messages\",\n",
    "    config=RunnableConfig(\n",
    "        callbacks=[cl.AsyncLangchainCallbackHandler()],\n",
    "        configurable=...,\n",
    "    ),\n",
    "):\n",
    "    print(f\"Msg: {msg}\")\n",
    "    print(f\"Metadata: {metadata}\")\n",
    "```\n",
    "\n",
    "### Synchronous API\n",
    "\n",
    "If simplicity is your priority, you can process the response in one go. This approach is easier to implement but may result in a slight delay before the user sees the response.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "response = agent.invoke(dict(messages=\"Some query\"))\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try synchronous API first\n",
    "response = agent.invoke(dict(messages=\"What is the capital of France?\"))\n",
    "print(\"Synchronous response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try async streaming API\n",
    "\n",
    "# Import chainlit for the AsyncLangchainCallbackHandler\n",
    "# This is needed for streaming responses in the Chainlit UI\n",
    "import chainlit as cl\n",
    "\n",
    "# Import RunnableConfig for the async streaming API\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "# This function demonstrates how streaming would work in the actual Chainlit application (chat_app.py)\n",
    "# It won't run in Jupyter because Chainlit's AsyncLangchainCallbackHandler requires the Chainlit application context\n",
    "# For testing in Jupyter, use the run_without_chainlit() function below instead\n",
    "async def test_streaming():\n",
    "    print(\"\\nStreaming response:\")\n",
    "    async for msg, metadata in agent.astream(\n",
    "        dict(messages=\"Tell me about Paris\"),\n",
    "        stream_mode=\"messages\",\n",
    "        config=RunnableConfig(callbacks=[cl.AsyncLangchainCallbackHandler()], configurable={}),\n",
    "    ):\n",
    "        print(f\"Msg: {msg}\")\n",
    "        print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the async function\n",
    "# This will fail with LookupError when not running in Chainlit context\n",
    "# Instead, let's use a simple callback handler that works in Jupyter\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "\n",
    "# Run the function without Chainlit callbacks\n",
    "async def run_without_chainlit():\n",
    "    print(\"\\nStreaming response:\")\n",
    "    async for msg, metadata in agent.astream(\n",
    "        dict(messages=\"Tell me about Paris\"),\n",
    "        stream_mode=\"messages\",\n",
    "        config=RunnableConfig(callbacks=[StdOutCallbackHandler()], configurable={}),\n",
    "    ):\n",
    "        print(f\"Msg: {msg}\")\n",
    "        print(f\"Metadata: {metadata}\")\n",
    "\n",
    "\n",
    "await run_without_chainlit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding memory and checkpointing\n",
    "\n",
    "Use `InMemorySaver` to checkpoint (remember) conversations.\n",
    "\n",
    "https://langchain-ai.github.io/langgraph/agents/memory/#short-term-memory\n",
    "\n",
    "It is crucial here to add a thread ID into the mix at this point, this ensure a proper chat history across user sessions\n",
    "\n",
    "- Check https://docs.chainlit.io/integrations/langchain#with-langgraph how to do this in Chainlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add tools\n",
    "\n",
    "- Start by adding a dummy tool (like `get_weather` as in the LangGraph tutorial, or `add = operator.add` to calculate addition)\n",
    "- Add a web search tool; DuckDuckGo is free to use, check https://python.langchain.com/docs/integrations/tools/ddg/\n",
    "- Look into more useful tools: https://langchain-ai.github.io/langgraph/agents/tools/#prebuilt-tools\n",
    "- Implement your own: https://langchain-ai.github.io/langgraph/agents/tools/#define-simple-tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
